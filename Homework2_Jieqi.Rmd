---
title: "Homework2"
author: "Jieqi Tu"
date: "9/28/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Question 1

#### (a)
Plot the log likelihood function.
```{r question 1 a plot}
# Import data
x_value = c(1.77, -0.23, 2.76, 3.80, 3.47, 
            56.75, -1.34, 4.24, -2.44, 3.29, 
            3.71, -2.40, 4.53, -0.07, -1.05, 
            -13.87, -2.53, -1.75, 0.27, 43.21)

# Define the log likelihood function
log_likelihood = function(x, theta) {
  l = -20*log(pi) - sum(log(1+(x-theta)^2))
  return(l)
} 

# Define the first derivative function of the log likelihood function
l_derivative_1 = function(x, theta) {
  l_1 = 2*sum((x-theta)/(1+(x-theta)^2))
  return(l_1)
}

# Define the second derivative function of the log likelihood function
l_derivative_2 = function(x, theta) {
  l_2 = 2*sum(((x-theta)^2-1)/(1+(x-theta)^2)^2)
  return(l_2)
}

# Graph the log likelihood function
theta_range = seq(-20, 20, 0.001)
n_loop = length(theta_range)
log_likelihood_value = numeric(n_loop)
for (i in 1:n_loop) {
  log_likelihood_value[i] = log_likelihood(x_value, theta_range[i])
}
result_l = cbind(theta_range, log_likelihood_value) %>% as.data.frame()
result_l %>% 
  ggplot(aes(x = theta_range, y = log_likelihood_value)) + geom_point(alpha = 0.01) +
  theme_bw() + labs(
    x = "theta",
    y = "log likelihood function",
    title = "Plot of log likelihood function"
  )
```

Find the MLE for theta using Newton-Raphson method.
```{r question 1 a mle}
n_iteration = 1000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = mean(x_value)
# Mean of the data as the starting point
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = -11
theta_iteration[1] = -11
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = -1
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 0
theta_iteration[1] = 0
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 1.5
theta_iteration[1] = 1.5
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 4
theta_iteration[1] = 4
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 4.7
theta_iteration[1] = 4.7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 7
theta_iteration[1] = 7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 8
theta_iteration[1] = 8
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 38
theta_iteration[1] = 38
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)
```

Discussion: the true theta would be around 0 and a little bit less than 0 (from the plot). So from our results, starting point -1 and 0 did a great job. Newton-Raphson method is very sensitive to the starting points. The mean of data is not a good starting point. We should set the starting point close to what we guess from the plot.

#### (b)
Use Bisection method with starting points -1 and 1.
```{r question 1 b}
# Starting points = -1 and 1
L = -1
U = 1
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)

# Starting points = 1 and 2
L = 1
U = 2
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)

# Starting points = 50 and 52
L = 50
U = 55
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)
```

Bisection method sometimes still fails to find the globlal maximum. Since it would be easier to get the local maximum near the starting points, sometimes it is possible to converge to a local maximum. Moreover, Bisection method might be slower than Newton-Raphson method.

#### (c)
Apply fixed-point iterations, starting from -1, with and without scaling.
```{r question 1 c}
# Define the g function
g_function = function(theta, alpha) {
  g_value = theta + alpha*l_derivative_1(x_value, theta)
  return(g_value)
}
# Without scaling (alpha = 1)
n_iteration = 2000000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 1)
}

tail(theta_iteration)

# Alpha = 0.64
n_iteration = 100000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.64)
}

tail(theta_iteration)


# Alpha = 0.25
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)

head(theta_iteration)
```

With the same starting point -1, the more shrinkage we have on the original function, the faster we can reach the convergence of theta.

```{r question 1 c addition}
# Alpha = 0.25, starting point = 0
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 0
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)
theta_iteration

# Alpha = 0.25, starting point = 2
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 2
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)
theta_iteration
```

Having the same scaling, convergence could be totally different with different starting point. So the choice of starting point is very important, and should be close to the global maximum.

#### (d)
Apply the secant method to estimate theta.
```{r question 1 d}
# Define the secant function
secant_function = function(theta1, theta2) {
  f_xi = l_derivative_1(x_value, theta2)
  f_xi_1 = l_derivative_1(x_value, theta1)
  theta_new = theta2 - f_xi*(theta2-theta1)/(f_xi-f_xi_1)
  return(theta_new)
}

# Starting points = -2 and -1
n_iteration = 10
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -2
theta_iteration[2] = -1
for (i in 3:n_iteration) {
  theta_iteration[i] = secant_function(theta_iteration[i-2], theta_iteration[i-1])
}
tail(theta_iteration)

# Starting points = -3 and 3
```

