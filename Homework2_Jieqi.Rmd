---
title: "Homework 2"
author: "Jieqi Tu"
date: "9/28/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(plotrix)
```

## Question 1

#### (a)
Plot the log likelihood function.
```{r question 1 a plot}
# Import data
x_value = c(1.77, -0.23, 2.76, 3.80, 3.47, 
            56.75, -1.34, 4.24, -2.44, 3.29, 
            3.71, -2.40, 4.53, -0.07, -1.05, 
            -13.87, -2.53, -1.75, 0.27, 43.21)

# Define the log likelihood function
log_likelihood = function(x, theta) {
  l = -20*log(pi) - sum(log(1+(x-theta)^2))
  return(l)
} 

# Define the first derivative function of the log likelihood function
l_derivative_1 = function(x, theta) {
  l_1 = 2*sum((x-theta)/(1+(x-theta)^2))
  return(l_1)
}

# Define the second derivative function of the log likelihood function
l_derivative_2 = function(x, theta) {
  l_2 = 2*sum(((x-theta)^2-1)/(1+(x-theta)^2)^2)
  return(l_2)
}

# Graph the log likelihood function
theta_range = seq(-20, 20, 0.001)
n_loop = length(theta_range)
log_likelihood_value = numeric(n_loop)
for (i in 1:n_loop) {
  log_likelihood_value[i] = log_likelihood(x_value, theta_range[i])
}
result_l = cbind(theta_range, log_likelihood_value) %>% as.data.frame()
result_l %>% 
  ggplot(aes(x = theta_range, y = log_likelihood_value)) + geom_line(alpha = 0.5) +
  theme_bw() + labs(
    x = "theta",
    y = "log likelihood function",
    title = "Plot of log likelihood function"
  )
```

Find the MLE for theta using Newton-Raphson method.
```{r question 1 a mle}
n_iteration = 1000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = mean(x_value)
# Mean of the data as the starting point
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = -11
theta_iteration[1] = -11
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = -1
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 0
theta_iteration[1] = 0
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 1.5
theta_iteration[1] = 1.5
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 4
theta_iteration[1] = 4
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 4.7
theta_iteration[1] = 4.7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 7
theta_iteration[1] = 7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 8
theta_iteration[1] = 8
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 38
theta_iteration[1] = 38
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)
```

Discussion: the true theta would be around 0 and a little bit less than 0 (from the plot). So from our results, starting point -1 and 0 did a great job. Newton-Raphson method is very sensitive to the starting points. The mean of data is not a good starting point. We should set the starting point close to what we guess from the plot.

#### (b)
Use Bisection method with starting points -1 and 1.
```{r question 1 b}
# Starting points = -1 and 1
L = -1
U = 1
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)

# Starting points = 1 and 2
L = 1
U = 2
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)

# Starting points = 50 and 52
L = 50
U = 55
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)
```

Bisection method sometimes still fails to find the globlal maximum. Since it would be easier to get the local maximum near the starting points, sometimes it is possible to converge to a local maximum. Moreover, Bisection method might be slower than Newton-Raphson method.

#### (c)
Apply fixed-point iterations, starting from -1, with and without scaling.
```{r question 1 c}
# Define the g function
g_function = function(theta, alpha) {
  g_value = theta + alpha*l_derivative_1(x_value, theta)
  return(g_value)
}
# Without scaling (alpha = 1)
n_iteration = 2000000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 1)
}

tail(theta_iteration)

# Alpha = 0.64
n_iteration = 100000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.64)
}

tail(theta_iteration)


# Alpha = 0.25
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)

head(theta_iteration)
```

With the same starting point -1, the more shrinkage we have on the original function, the faster we can reach the convergence of theta.

```{r question 1 c addition}
# Alpha = 0.25, starting point = 0
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 0
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)
theta_iteration

# Alpha = 0.25, starting point = 2
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 2
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)
theta_iteration
```

Having the same scaling, convergence could be totally different with different starting point. So the choice of starting point is very important, and should be close to the global maximum.

#### (d)
Apply the secant method to estimate theta.
```{r question 1 d}
# Define the secant function
secant_function = function(theta1, theta2) {
  f_xi = l_derivative_1(x_value, theta2)
  f_xi_1 = l_derivative_1(x_value, theta1)
  theta_new = theta2 - f_xi*(theta2-theta1)/(f_xi-f_xi_1)
  return(theta_new)
}

# Starting points = -2 and -1
n_iteration = 10
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -2
theta_iteration[2] = -1
for (i in 3:n_iteration) {
  theta_iteration[i] = secant_function(theta_iteration[i-2], theta_iteration[i-1])
}
tail(theta_iteration)

# Starting points = -3 and 3
n_iteration = 10
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -3
theta_iteration[2] = 3
for (i in 3:n_iteration) {
  theta_iteration[i] = secant_function(theta_iteration[i-2], theta_iteration[i-1])
}
tail(theta_iteration)

# Starting points = 50 and 55
n_iteration = 10
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 50
theta_iteration[2] = 55
for (i in 3:n_iteration) {
  theta_iteration[i] = secant_function(theta_iteration[i-2], theta_iteration[i-1])
}
tail(theta_iteration)
```

We get to the true convergence when starting points = -2 and -1. When the starting points are -3 and 3, or 50 and 55, we got the local maximum. So the starting points should be chosen carefully.

#### (e)
Compare the speed and stability of these methods.

* Newton-Raphson method is usually the fatest method than others, but it is also very sensitive to the starting points.
* Fixed point method is the most stable one but it is slower, the scaling can ajust the speed to convergence.
* Bisection method is also stable and slower.
* Secant is also fast. However, after reaching convergence, the theta might becomes NaN, due to the zero in denominator.

## Question 2
```{r question 2 data import}
# Import data
x_observed = c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99,
2.54, 0.52, 2.50)

# Define the log likelihood function
l_function = function(x, theta) {
  l_value = -20*log(2*pi) + sum(log(1-cos(x-theta)))
  return(l_value)
}

# Define the 1st derivative of the log likelihood function
l_derivative_1 = function(x, theta) {
  l_der_value = -sum(sin(x-theta)/(1-cos(x-theta)))
  return(l_der_value)
}

# Define the 2nd derivative of the log likelihood function
l_derivative_2 = function(x, theta) {
  l_second_der = -sum(1/(1-cos(x-theta)))
  return(l_second_der)
}
```

#### (a)
Plot the log likelihood function and the 1st derivative of the log likelihood function between $-\pi$ and $\pi$.
```{r question 2 a plot}
x_range = seq(-pi, pi, 0.01)
l_value = numeric(length(x_range))
l_derivative = numeric(length(x_range))
for (i in 1:length(x_range)) {
  l_value[i] = l_function(x_observed, x_range[i])
  l_derivative[i] = l_derivative_1(x_observed, x_range[i])
}
result = cbind(x_range, l_value, l_derivative) %>% as.data.frame()

# Plot the log likelihood function
result %>% 
  ggplot(aes(x = x_range, y = l_value)) + geom_line(alpha= 0.5) +
  theme_bw() + labs(
    x = "theta",
    y = "log likelihood function",
    title = "Plot of log likelihood function"
  )

# Plot the derivative of log likelihood function
result %>% 
  ggplot(aes(x = x_range, y = l_derivative)) + geom_line(alpha= 0.5) +
  theme_bw() + labs(
    x = "theta",
    y = "first derivative of log likelihood function",
    title = "Plot of 1st derivative of log likelihood function"
  )
```

#### (b)
Find the method-of-moments estimator of $\theta$.
The first theoretical moment about the origin is: $E(X)=\int^{\infty}_{-\infty}xf(x)dx=\int^{2\pi}_{0}x(1-cos(x-\theta))/2\pi dx=\frac{1}{2\pi}(sin(\theta)+2\pi^{2})$.
Therefore, we have $sin\theta=E(X)-\pi$. So the method-of-moments estimator would be $\hat{\theta}=\arcsin((E(X)-\pi))$.
```{r question 2 b}
# MME of theta
theta_mme = asin((mean(x_observed)-pi));theta_mme
```


#### (c)
Find the MLE for $\theta$ using the Newton-Raphson method.
```{r question 2 c}
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = theta_mme
# MME of theta as the starting point
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_observed, theta_iteration[i-1])/l_derivative_2(x_observed, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = -2.7
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -2.7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_observed, theta_iteration[i-1])/l_derivative_2(x_observed, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 2.7
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 2.7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_observed, theta_iteration[i-1])/l_derivative_2(x_observed, theta_iteration[i-1])
}
tail(theta_iteration)
```

When the starting value is MME, it converges to -0.011972.
When the starting value is -2.7, it converges to -2.6667.
When the starting value is 2.7, it converges to 2.873095.

#### (d)
Repeat part (c) using 200 equally spaced starting values between $-\pi$ and $\pi$.
```{r question 2 d}
# Create starting values
start_values = seq(-pi, pi, length = 200)

# Newton-Raphson method for each starting values
convergence = numeric(200)
n_iteration = 100
theta_iteration = numeric(n_iteration)
for (j in 1:200) {
  # Set starting value
  theta_iteration[1] = start_values[j]
  # Find the convergence theta value
  for (i in 2:n_iteration) {
    theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_observed, theta_iteration[i-1])/l_derivative_2(x_observed, theta_iteration[i-1])
  }
  convergence[j] = theta_iteration[100]
}

result_list = cbind(start_values, convergence)
result_list %>% knitr::kable()
```

Discussion: Different starting values have different convergence of theta. However, only starting values from -0.8051318 to 0.4893938 lead to the true maximum.

#### (e)
Find two starting values, as nearly equal as you can, for which the Newtonâ€“Raphson method converges to two different solutions.
From part (d), we could know that, we can split the range of [$-\pi,\pi$] as much as possible. We could assign the space as small as possible.

## Question 3
#### (a)
Use fixed point iteration with $G_1(x)$.
```{r question 3 a}
# Define g'(x)
g_x_derivative = function(epsilon, x) {
  g_x_value = 1628/x - 1013 - 3062*(1-epsilon)*exp(-x)/(epsilon+(1-epsilon)*exp(-x))
  return(g_x_value)
}

# Define G1(x)
G1_x = function(epsilon, x) {
  G1_value = 1628*(epsilon+(1-epsilon)*exp(-x))/(3062*(1-epsilon)*exp(-x)+1013*(epsilon+(1-epsilon)*exp(-x)))
  return(G1_value)
}

# Plot the g'(x)
x_range = seq(-20, 20, 0.1)
g_prime_x = numeric(length(x_range))
for (i in 1:length(x_range)) {
  g_prime_x[i] = g_x_derivative(0.61489, x_range[i])
}
result = cbind(x_range, g_prime_x) %>% as.data.frame()
result %>% 
  ggplot(aes(x = x_range, y = g_prime_x)) + geom_line(alpha = 0.5) +
  theme_bw()

# Fixed point iteration with G1(x)
n_iteration = 20
x_list = numeric(n_iteration)
x_list = 1 # Starting value is 1
for (i in 2:n_iteration) {
  x_list[i] = G1_x(0.61489, x_list[i-1])
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

Using fixed point iteration, $G_1(x)$ converges to 1.037536 with starting value 1.

#### (b)
Demonstrate that $G_2(x)=x+g'(x)$ fails to converge.
```{r question 3 b}
# Define G2(x)
G2_x = function(epsilon, x) {
  G2_value = x + 1628/x - 1013 - 3062*(1-epsilon)*exp(-x)/(epsilon+(1-epsilon)*exp(-x))
  return(G2_value)
}

# Fixed point iteration with G2(x)
n_iteration = 5
x_list = numeric(n_iteration)
x_list[1] = 1 # Starting value = 1
for (i in 2:n_iteration) {
  x_list[i] = G2_x(0.61489, x_list[i-1])
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

The x values become NaN after a few iterations. So it is not convergent to some value with starting point 1.

#### (c)
Try $G_3(x)=x+\alpha g'(x),\alpha=1/1000$.
```{r question 3 c}
# Define G3(x)
G3_x = function(epsilon, x, alpha) {
  G3_value = x + alpha*(1628/x - 1013 - 3062*(1-epsilon)*exp(-x)/(epsilon+(1-epsilon)*exp(-x)))
  return(G3_value)
}

# Fixed point iteration with scaling
n_iteration = 10
x_list = numeric(n_iteration)
x_list[1] = 1
for (i in 2:n_iteration) {
  x_list[i] = G3_x(0.61489, x_list[i-1], 0.001)
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

After scaling with $\alpha=0.001$, it converges to 1.037536.

#### (d)
Newton-Raphson method using starting value 3.
```{r question 3 d}
# Define the second derivative of g(x)
g_x_derivative_2 = function(epsilon, x) {
  second_deri_value = 3062*(1-epsilon)*epsilon*exp(-x)/(epsilon+(1-epsilon)*exp(-x))^2-1628/x^2
  return(second_deri_value)
}

# Newton-Raphson iteration with starting value 3
n_iteration = 5
x_list = numeric(n_iteration)
x_list[1] = 3
for (i in 2:n_iteration) {
  x_list[i] = x_list[i-1] - g_x_derivative(0.61489, x_list[i-1])/g_x_derivative_2(0.61489, x_list[i-1])
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

#### (e)
Use Newton-Raphson method with starting value = 1.5.
```{r question 3 e}
n_iteration = 10
x_list = numeric(n_iteration)
x_list[1] = 1.5 # Starting value = 1.5
for (i in 2:n_iteration) {
  x_list[i] = x_list[i-1] - g_x_derivative(0.61489, x_list[i-1])/g_x_derivative_2(0.61489, x_list[i-1])
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

#### (f)
Use secant method with starting value 1.5 and 1.49.
```{r question 3 f}
# Define the secant function
secant_function = function(x_1, x_2) {
  f_xi = g_x_derivative(0.61489, x_2)
  f_xi_1 = g_x_derivative(0.61489, x_1)
  x_new = x_2 - f_xi*(x_2-x_1)/(f_xi-f_xi_1)
  return(x_new)
}

# Secant method with starting value = 1.5 and 1.49
n_iteration = 10
x_list = numeric(n_iteration)
x_list[1] = 1.5
x_list[2] = 1.49
for (i in 3:n_iteration) {
  x_list[i] = secant_function(x_list[i-2], x_list[i-1])
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

#### (g)
Use Muller's method. Choose your third starting value as 1.48.
```{r question 3 g}
# Initialization
n_iteration = 10
x_list = numeric(n_iteration)
x_list[1] = 1.5
x_list[2] = 1.49
x_list[3] = 1.48

# Define Muller's function
muller_function = function(x1,x2,x3) {
  f_i_2 = g_x_derivative(0.61489, x1)
  f_i_1 = g_x_derivative(0.61489, x2)
  f_i = g_x_derivative(0.61489, x3)
  dd = (f_i - 2*f_i_1 + f_i_2)/(x3 - x1)
  s = f_i - f_i_1 + (x3 - x1)/dd
  q = (x3-x2)/(x2-x1)
  A = q*f_i - q*(1+q)*f_i_1 + q^2*f_i_2
  B = (2*q+1)*f_i-(1+q)^2*f_i_1+q^2*f_i_2
  C = (1+q)*f_i
  x4 = x3-(x3-x2)*2*C/(B+sign(s)*sqrt(B^2-4*A*C))
  return(x4)
}

# Iterations
for (i in 4:n_iteration) {
  x_list[i] = muller_function(x_list[i-3], x_list[i-2], x_list[i-1])
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

#### (h)
Use bisection method. The starting values would be 1 and 2.
```{r question 3 h}
L = 1
U = 2
n_iteration = 25
x_list = numeric(n_iteration)
for (i in 1:n_iteration) {
  x_list[i] = (L + U)/2
  eval_1 = g_x_derivative(0.61489, x_list[i])
  eval_2 = g_x_derivative(0.61489, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = x_list[i]
  } else {L = x_list[i]}
}

g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

#### (i)
Use secant-bracket method. The starting values are 1 and 2.
```{r question 3 i}
n_iteration = 10
x_list = numeric(n_iteration)
L = 1 # x0
U = 2 # x1
for (i in 1:n_iteration) {
  x_list[i] = (L*g_x_derivative(0.61489, U)-U*g_x_derivative(0.61489, L))/(g_x_derivative(0.61489, U) - g_x_derivative(0.61489, L))
  eval1 = g_x_derivative(0.61489, x_list[i])
  eval2 = g_x_derivative(0.61489, L)
  eval3 = eval1*eval2
  if (eval3<0) {
    U = x_list[i]
  } else {
    L = x_list[i]
  }
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

#### (j)
Use Illinois method. Starting values = 1 and 2
```{r question 3 j}
n_iteration = 10
x_list = numeric(n_iteration)
L = 1 # x0
U = 2 # x1
x_list[1] = U  
FL = g_x_derivative(0.61489, L)
FU = g_x_derivative(0.61489, U)
for (i in 2:n_iteration) {
  x_list[i] = (L*FU - U*FL)/(FU-FL)
  eval = g_x_derivative(0.61489, x_list[i])*FL
  
  if(eval < 0) {
    U = x_list[i]
    FU = g_x_derivative(0.61489, U)
    if (FU*g_x_derivative(0.61489, x_list[i-1])>0) {
      FL = FL/2
    }
  } else {
    L = x_list[i]
    FL = g_x_derivative(0.61489, L)
    if (FL*g_x_derivative(0.61489, x_list[i-1])>0) {
      FU = FU/2
    }
  }
}
g_prime_x=g_x_derivative(0.61489, x_list)
result = cbind(x_list, g_prime_x) %>% as.data.frame()
result %>% knitr::kable()
```

## Question 4

#### (a)
```{r import data}
# Import data
m = c(1997, 906, 904, 32)
sample_size = sum(m_values); sample_size
```

```{r question 4 a plot}
# Define the log likelihood function
log_L = function(theta) {
  log_L_value = m[1]*log((2+theta)/4)+(m[2]+m[3])*log((1-theta)/4)+m[4]*log(theta/4)
  return(log_L_value)
}

theta_range = seq(0, 1, 0.01)
log_L_plot_value = numeric(length(theta_range))
for (i in 1:length(theta_range)) {
  log_L_plot_value[i] = log_L(theta_range[i])
}
plot_variables = cbind(theta_range, log_L_plot_value) %>% as.data.frame()
# Plot the log likelihood function of theta
plot_variables %>% 
  ggplot(aes(x = theta_range, y = log_L_plot_value)) + geom_line(alpha = 0.4) +
  theme_bw() + labs(
    x = "theta",
    y = "log likelihood function"
  )
```

From the plot, we could see that the global maximum is a bit greater than 0.

Use Newton-Raphsom method to estimate theta.
```{r question 4 a NR}
# Define the 1st derivative of log likelihood function
l_derivative_1 = function(theta) {
  derivative1 = 4*(m[1]/(2+theta) -(m[2]+m[3])/(1-theta) + m[4]/theta)
  return(derivative1)
}

# Define the 2nd derivative of log likelihood function
l_derivative_2 = function(theta) {
  derivative2 = 4*(-m[1]/(2+theta)^2-(m[2]+m[3])/(1-theta)^2-m[4]/theta^2)
  return(derivative2)
}

# Define the Fisher function
l_derivative_2_Fisher = function(theta) {
  m_expected = sample_size * c((2+theta)/4, (1-theta)/4, (1-theta)/4, theta/4)
  fisher = 4*(m_expected[1]/(2+theta)^2+(m_expected[2]+m_expected[3])/(1-theta)^2+m_expected[4]/theta^2)
  return(fisher)
}

n_iteration = 7
theta_list = numeric(n_iteration)
theta_list[1] = 0.0570461 # Starting value = 0.0570461
for (i in 2:n_iteration) {
  theta_list[i] = theta_list[i-1] - l_derivative_1(theta_list[i-1])/l_derivative_2(theta_list[i-1])
}
score_values = l_derivative_1(theta_list)
obs_Hessian = -l_derivative_2(theta_list)
exp_Hessian = l_derivative_2_Fisher(theta_list)
result = cbind(theta_list, score_values, obs_Hessian, exp_Hessian) %>% as.data.frame()
result %>% knitr::kable()

# Calculate the standard error
var_NR = (exp_Hessian[n_iteration])^(-1)
std_error_NR = sqrt(var_NR/sample_size);std_error_NR
```

Use Fisher scoring to estimate theta with the same starting value.
```{r question 4 a Fisher}
n_iteration = 10
theta_list = numeric(n_iteration)
theta_list[1] = 0.0570461 # Starting value = 0.0570461
for (i in 2:n_iteration) {
  theta_list[i] = theta_list[i-1] + l_derivative_1(theta_list[i-1])/l_derivative_2_Fisher(theta_list[i-1])
}
score_values = l_derivative_1(theta_list)
obs_Hessian = -l_derivative_2(theta_list)
exp_Hessian = l_derivative_2_Fisher(theta_list)
result = cbind(theta_list, score_values, obs_Hessian, exp_Hessian) %>% as.data.frame()
result %>% knitr::kable()

# Calculate the standard error
var_fisher = (exp_Hessian[n_iteration])^(-1)
std_error_fisher = sqrt(var_fisher/sample_size);std_error_fisher
```


#### (b)
Use Newton-Raphson method using starting value 0.5.
```{r}

```

