---
title: "Homework2"
author: "Jieqi Tu"
date: "9/28/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Question 1

#### (a)
Plot the log likelihood function.
```{r question 1 a plot}
# Import data
x_value = c(1.77, -0.23, 2.76, 3.80, 3.47, 
            56.75, -1.34, 4.24, -2.44, 3.29, 
            3.71, -2.40, 4.53, -0.07, -1.05, 
            -13.87, -2.53, -1.75, 0.27, 43.21)

# Define the log likelihood function
log_likelihood = function(x, theta) {
  l = -20*log(pi) - sum(log(1+(x-theta)^2))
  return(l)
} 

# Define the first derivative function of the log likelihood function
l_derivative_1 = function(x, theta) {
  l_1 = 2*sum((x-theta)/(1+(x-theta)^2))
  return(l_1)
}

# Define the second derivative function of the log likelihood function
l_derivative_2 = function(x, theta) {
  l_2 = 2*sum(((x-theta)^2-1)/(1+(x-theta)^2)^2)
  return(l_2)
}

# Graph the log likelihood function
theta_range = seq(-20, 20, 0.001)
n_loop = length(theta_range)
log_likelihood_value = numeric(n_loop)
for (i in 1:n_loop) {
  log_likelihood_value[i] = log_likelihood(x_value, theta_range[i])
}
result_l = cbind(theta_range, log_likelihood_value) %>% as.data.frame()
result_l %>% 
  ggplot(aes(x = theta_range, y = log_likelihood_value)) + geom_line(alpha = 0.5) +
  theme_bw() + labs(
    x = "theta",
    y = "log likelihood function",
    title = "Plot of log likelihood function"
  )
```

Find the MLE for theta using Newton-Raphson method.
```{r question 1 a mle}
n_iteration = 1000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = mean(x_value)
# Mean of the data as the starting point
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = -11
theta_iteration[1] = -11
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = -1
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 0
theta_iteration[1] = 0
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 1.5
theta_iteration[1] = 1.5
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 4
theta_iteration[1] = 4
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 4.7
theta_iteration[1] = 4.7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 7
theta_iteration[1] = 7
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 8
theta_iteration[1] = 8
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)

# Starting point = 38
theta_iteration[1] = 38
for (i in 2:n_iteration) {
  theta_iteration[i] = theta_iteration[i-1] - l_derivative_1(x_value, theta_iteration[i-1])/l_derivative_2(x_value, theta_iteration[i-1])
}
tail(theta_iteration)
```

Discussion: the true theta would be around 0 and a little bit less than 0 (from the plot). So from our results, starting point -1 and 0 did a great job. Newton-Raphson method is very sensitive to the starting points. The mean of data is not a good starting point. We should set the starting point close to what we guess from the plot.

#### (b)
Use Bisection method with starting points -1 and 1.
```{r question 1 b}
# Starting points = -1 and 1
L = -1
U = 1
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)

# Starting points = 1 and 2
L = 1
U = 2
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)

# Starting points = 50 and 52
L = 50
U = 55
n_iteration = 100000
theta_iteration = numeric(n_iteration)
for (i in 1:n_iteration) {
  theta_iteration[i] = (L + U)/2
  eval_1 = l_derivative_1(x_value, theta_iteration[i])
  eval_2 = l_derivative_1(x_value, L)
  eval_3 = eval_1 * eval_2
  if(eval_3 < 0) {
    U = theta_iteration[i]
  } else {L = theta_iteration[i]}
}

tail(theta_iteration)
```

Bisection method sometimes still fails to find the globlal maximum. Since it would be easier to get the local maximum near the starting points, sometimes it is possible to converge to a local maximum. Moreover, Bisection method might be slower than Newton-Raphson method.

#### (c)
Apply fixed-point iterations, starting from -1, with and without scaling.
```{r question 1 c}
# Define the g function
g_function = function(theta, alpha) {
  g_value = theta + alpha*l_derivative_1(x_value, theta)
  return(g_value)
}
# Without scaling (alpha = 1)
n_iteration = 2000000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 1)
}

tail(theta_iteration)

# Alpha = 0.64
n_iteration = 100000
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.64)
}

tail(theta_iteration)


# Alpha = 0.25
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -1
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)

head(theta_iteration)
```

With the same starting point -1, the more shrinkage we have on the original function, the faster we can reach the convergence of theta.

```{r question 1 c addition}
# Alpha = 0.25, starting point = 0
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 0
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)
theta_iteration

# Alpha = 0.25, starting point = 2
n_iteration = 100
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 2
for (i in 2:n_iteration) {
  theta_iteration[i] = g_function(theta_iteration[i-1], 0.25)
}

tail(theta_iteration)
theta_iteration
```

Having the same scaling, convergence could be totally different with different starting point. So the choice of starting point is very important, and should be close to the global maximum.

#### (d)
Apply the secant method to estimate theta.
```{r question 1 d}
# Define the secant function
secant_function = function(theta1, theta2) {
  f_xi = l_derivative_1(x_value, theta2)
  f_xi_1 = l_derivative_1(x_value, theta1)
  theta_new = theta2 - f_xi*(theta2-theta1)/(f_xi-f_xi_1)
  return(theta_new)
}

# Starting points = -2 and -1
n_iteration = 10
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -2
theta_iteration[2] = -1
for (i in 3:n_iteration) {
  theta_iteration[i] = secant_function(theta_iteration[i-2], theta_iteration[i-1])
}
tail(theta_iteration)

# Starting points = -3 and 3
n_iteration = 10
theta_iteration = numeric(n_iteration)
theta_iteration[1] = -3
theta_iteration[2] = 3
for (i in 3:n_iteration) {
  theta_iteration[i] = secant_function(theta_iteration[i-2], theta_iteration[i-1])
}
tail(theta_iteration)

# Starting points = 50 and 55
n_iteration = 10
theta_iteration = numeric(n_iteration)
theta_iteration[1] = 50
theta_iteration[2] = 55
for (i in 3:n_iteration) {
  theta_iteration[i] = secant_function(theta_iteration[i-2], theta_iteration[i-1])
}
tail(theta_iteration)
```

We get to the true convergence when starting points = -2 and -1. When the starting points are -3 and 3, or 50 and 55, we got the local maximum. So the starting points should be chosen carefully.

#### (e)
Compare the speed and stability of these methods.

* Newton-Raphson method is usually the fatest method than others, but it is also very sensitive to the starting points.
* Fixed point method is the most stable one but it is slower, the scaling can ajust the speed to convergence.
* Bisection method is also stable and slower.
* Secant is also fast. However, after reaching convergence, the theta might becomes NaN, due to the zero in denominator.

## Question 2
```{r question 2 data import}
# Import data
x_observed = c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99,
2.54, 0.52, 2.50)

# Define the log likelihood function
l_function = function(x, theta) {
  l_value = -20*log(2*pi) + sum(log(1-cos(x-theta)))
  return(l_value)
}

# Define the 1st derivative of the log likelihood function
l_derivative_1 = function(x, theta) {
  l_der_value = -sum(sin(x-theta)/(1-cos(x-theta)))
  return(l_der_value)
}

# Define the 2nd derivative of the log likelihood function
l_derivative_2 = function(x, theta) {
  l_second_der = sum(1/(1-cos(x-theta)))
  return(l_second_der)
}
```

#### (a)
Plot the log likelihood function and the 1st derivative of the log likelihood function between $-\pi$ and $\pi$.
```{r question 1 a plot}
x_range = seq(-pi, pi, 0.01)
l_value = numeric(length(x_range))
l_derivative = numeric(length(x_range))
for (i in 1:length(x_range)) {
  l_value[i] = l_function(x_observed, x_range[i])
  l_derivative[i] = l_derivative_1(x_observed, x_range[i])
}
result = cbind(x_range, l_value, l_derivative) %>% as.data.frame()

# Plot the log likelihood function
result %>% 
  ggplot(aes(x = x_range, y = l_value)) + geom_line(alpha= 0.5) +
  theme_bw() + labs(
    x = "theta",
    y = "log likelihood function",
    title = "Plot of log likelihood function"
  )

# Plot the log likelihood function
result %>% 
  ggplot(aes(x = x_range, y = l_derivative)) + geom_line(alpha= 0.5) +
  theme_bw() + labs(
    x = "theta",
    y = "first derivative of log likelihood function",
    title = "Plot of 1st derivative of log likelihood function"
  )
```

#### (b)
Find the method-of-moments estimator of $\theta$.
The first theoretical moment about the origin is: $E(X)=\int^{\infty}_{-\infty}xf(x)dx=\int^{2\pi}_{0}x(1-cos(x-\theta))/2\pi dx=\frac{1}{2\pi}(sin(\theta)+2\pi^{2})$.
Therefore, we have $sin\theta=2\pi(E(X)-\pi)$. So the method-of-moments estimator would be $\hat{\theta}=\arcsin(2\pi(E(X)-\pi))$.
```{r question 2 b}
# MME of theta
theta_mme = asin(2*pi*(mean(x_observed)-pi));theta_mme
```


#### (c)
Find the MLE for $\theta$ using the Newton-Raphson method.